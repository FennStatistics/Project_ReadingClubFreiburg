
---
title: "Chapter 1, 2"
subtitle: "of Farrell & Lewandowsky 2018"
author: "Julius Fenn for"
institute: "the Reading Club at University of Freiburg"
date: "Freiburg, `r Sys.Date()`"
output:
  xaringan::moon_reader:
    includes:
      after_body: "./HTML/insert-logo.html"
    css: ["default", "CSS/freiburg.css", "CSS/freiburg-fonts.css"]
    lib_dir: libs
    nature:
      titleSlideClass: ["title-slide"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

class: center, middle

## xaringan

### /ʃaː.'riŋ.ɡan/

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr/), and [R Markdown](https://rmarkdown.rstudio.com). 

Thanks to Raphael Hartmann [uf_xaringan](https://github.com/RaphaelHartmann/uf_xaringan)


---
class: inverse, center, middle

# Get the Reading Club Started!

---
class: overview
## Overview

* Chapter 1 - Models and Theories in Science
  + effect of practice
  + Generalized Context Model
  + relationship theory and data
  
* communication using models
  + time-based resource-sharing theory

* Chapter 2 - Diffusion Models
  + random walk model
  + consider trial-to-trial variability
  

---
class: inverse, center, middle

# Chapter 1 - Models and Theories in Science

---

## effect of practice


effect of practice is best captured by a “Power law” 

$$ RT = N^{-\beta}$$
* $RT$ represents the time to perform the task
* $N$ represents the number of learning trials to date
* $\beta$ is the learning rate


.pull-left[
```{r}
N <- 1:300
b <- .1
RT <- N^(-b)
```

> data are better described by an exponential function

]

.pull-right[
```{r, fig.height=4, fig.width=6, fig.align='center'}
plot(N, RT, xlab = "Trial Number")
```
]



---

## Generalized Context Model

The Generalized Context Model (GCM) is a cognitive model in psychology that explains how people make decisions under uncertainty by integrating information from multiple sources. It posits that people represent their knowledge in terms of features, and the likelihood of an outcome is determined by the similarity between the features of the current situation and those of previously encountered situations. The GCM provides a computational framework for understanding judgment and decision-making processes.

<img src="IMGs/GCM.jpg" alt="GCM" width="700"/>



---

## Generalized Context Model - formula

distance ($B$) between two stimuli can be computed as

$$d_{ij}=\sqrt{\sum_{k=1}^K|x_{ik}-x_{jk}|^2}$$
* $x_{ik}$ is the value of dimension k for test item i 
* $x_{jk}$ is the value of dimension k for the stored exemplar j

exponential relationship from distance to similarity $s_{ij}$

$$s_{ij}=exp(-c*d_{ij})$$
---

## relationship theory and data

* any theory that predicts any possible outcome with equal facility is of little scientific interest, even if it happens to be in complete accord with the data
* **falsifiable** -> we want to be assured that there are at least hypothetical outcomes that, if they are ever observed, would falsify a theory

.pull-left[
<img src="IMGs/relationTheoryData.jpg" alt="GCM" width="600" height="400"/>
]

.pull-right[

The top row of panels represents some hypothetical theory whose predictions are constrained to a narrow range of outcomes; any outcome outside the dotted sliver would constitute contrary evidence, and only the narrow range of values within the sliver would constitute supporting evidence.

> we would want our theories to occupy only a small region of the outcome space, but for all observed outcomes to fall within that region

]


---
class: inverse, center, middle

# communication using models


---

## communication using models

* communication among scientists resembles a game of “telephone” (also known as “Chinese whispers”), whereby theories and models are formulated by one researcher and recorded on paper before being read by the next scientist(s) who need(s) to understand them. Those new ideas may in turn be recorded in a further paper and so on. Each step in this chain involves cognitive reasoning, and is thus subject to the known limitations of human cognition – from our limited attentional capacity to the confirmation bias, to name but two
  + choice of analogy will affect a scientist’s understanding of her own model – will undoubtedly be compounded when theorizing involves groups of scholars who communicate with each other
* **using computational models in preference to verbal theorizing**

<br>
* Annes / Raphaels remark: complex formulas cannot be understood without sufficient training (e.g., in physics or maths to understand the diffusion model) -> both is necessary (model to specify and able to communicate verbally)
  
<br>
**Example:**
Oberauer, K., & Lewandowsky, S. (2011). Modeling working memory: A computational implementation of the Time-Based Resource-Sharing theory. Psychonomic Bulletin & Review, 18(1), 10–45. https://doi.org/10.3758/s13423-010-0020-6


---
class: inverse, center, middle

# Chapter 2 - Diffusion Models


---

## materials

all R Code / additional materials can be found on: 
https://psy-farrell.github.io/computational-modelling/


---

## random walk model

```{r}
set.seed(111)

#random walk model 
nreps <- 10000
nsamples <- 2000

drift <- 0  # 0 = non-informative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 

latencies <- rep(0,nreps)  
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  evidence[i,] <- cumsum(c(0,rnorm(nsamples,drift,sdrw)))  
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```




---

## plot random walk model

```{r, fig.height=4, fig.width=6, fig.align='center'}
#plot up to 5 random walk paths
tbpn <- min(nreps,5)
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
     ylim=c(-criterion-.5,criterion+.5),
     ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
  lines(evidence[i,1:(latencies[i]-1)])   
}
abline(h=c(criterion,-criterion),lty="dashed")  
```



---

## plot histograms of latencies


.pull-left[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "right"
botrt <- latencies[responses>0]; sum(responses>0)
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(botprop),
                ") m=",as.character(signif(mean(botrt),4)),
                sep=""),las=1)
```

]

.pull-right[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "wrong"
botrt <- latencies[responses<0]; sum(responses<0)
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
                ") m=",as.character(signif(mean(botrt),4)),
                sep=""),las=1)
```
]


---

## consider trial-to-trial variability

**variability in starting point**

```{r}
set.seed(222)
#random walk model with unequal latencies between responses classes
nreps <- 1000
nsamples <- 2000

drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c(2,0.0) # !!!

latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) 
  dr <- rnorm(1,drift,t2tsd[2]) 
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```



---

## plot histograms of latencies


.pull-left[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "right"
toprt <- latencies[responses>0]; sum(responses>0)
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
                ") m=",as.character(signif(mean(toprt),4)),
                sep=""),las=1)
```

]

.pull-right[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "wrong"
botrt <- latencies[responses<0]; sum(responses<0)
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
                ") m=",as.character(signif(mean(botrt),4)),
                sep=""),las=1)
```
]



---

## consider trial-to-trial variability

**variability in drift rate**; Annes remark: in theory discussed as fast error responses (avoidable errors)

```{r}
set.seed(222)
#random walk model with unequal latencies between responses classes
nreps <- 10000
nsamples <- 2000

drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c (0.0,0.025) # !!!

latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) 
  dr <- rnorm(1,drift,t2tsd[2]) 
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```



---

## plot histograms of latencies


.pull-left[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "right"
toprt <- latencies[responses>0]; sum(responses>0)
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
                ") m=",as.character(signif(mean(toprt),4)),
                sep=""),las=1)
```

]

.pull-right[

```{r, fig.height=3, fig.width=4, fig.align='center'}
#plot histograms of latencies - "wrong"
botrt <- latencies[responses<0]; sum(responses<0)
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
                ") m=",as.character(signif(mean(botrt),4)),
                sep=""),las=1)
```
]



---

## A Family of Possible Sequential-Sampling Models

<img src="IMGs/sequentialSamplingModels.jpg" alt="GCM" width="700"/>

Ratcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007
